{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.73s get data package ：）\n",
      "X & y prepared\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "from feature_extraction import __get\n",
    "\n",
    "path = r'/Users/loewi/Documents/Pre_Learn/classification/20news-bydate/'\n",
    "os.chdir(path)\n",
    "#print(os.getcwd())\n",
    "\n",
    "t0 = time() \n",
    "\n",
    "newsgroups_train = __get('20news-bydate-train')\n",
    "newsgroups_test = __get('20news-bydate-test')\n",
    "\n",
    "duration = time() - t0\n",
    "print('%0.2fs get data package ：）'%duration)\n",
    "\n",
    "X_train, X_test = newsgroups_train['data'], newsgroups_test['data']\n",
    "y_train, y_test = newsgroups_train['docs'], newsgroups_test['docs']\n",
    "print('X & y prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer,TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold,GridSearchCV,RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__kernel': ('linear', 'rbf')}\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  4.4min remaining:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 465.245s\n",
      "\n",
      "Best score: 0.619\n",
      "Best parameters set:\n",
      "\tclf__kernel: 'linear'\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vect', HashingVectorizer(\n",
    "                        lowercase = True,\n",
    "                        stop_words= 'english',\n",
    "                        token_pattern= r'(?u)\\b[a-zA-Z_][a-zA-Z_]+\\b',\n",
    "                        n_features = 10000\n",
    "                        )),\n",
    "                \n",
    "                ('clf', SVC( )),\n",
    "                ])\n",
    "parameters = { \n",
    "                'clf__kernel': ('linear', 'rbf'),\n",
    "                }    \n",
    "#SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "#  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#  tol=0.001, verbose=False)\n",
    "\n",
    "cv = StratifiedKFold(n_splits= 5 ,random_state=0,shuffle=True)\n",
    "clf = GridSearchCV(pipeline, parameters,n_jobs=-1, verbose=1, cv = 5)\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % clf.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.619 (std: 0.006)\n",
      "Parameters: {'clf__kernel': 'linear'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.062 (std: 0.018)\n",
      "Parameters: {'clf__kernel': 'rbf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "            \n",
    "report(clf.cv_results_)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.1, 1, 10, 100), 'clf__gamma': (1, 0.1, 0.01, 0.001, 1e-05, 10)}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 66.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4134.255s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vect', HashingVectorizer(\n",
    "                        lowercase = True,\n",
    "                        stop_words= 'english',\n",
    "                        token_pattern= r'(?u)\\b[a-zA-Z_][a-zA-Z_]+\\b',\n",
    "                        n_features = 10000\n",
    "                        )),\n",
    "                \n",
    "                ('clf', SVC( \n",
    "                    kernel = 'rbf',\n",
    "                )),\n",
    "                ])\n",
    "parameters = { \n",
    "                'clf__gamma': (1, 0.1, 0.01, 0.001, 0.00001, 10),\n",
    "                'clf__C': (0.1, 1, 10, 100),\n",
    "                }    \n",
    "\n",
    "#SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "#  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#  tol=0.001, verbose=False)\n",
    "\n",
    "cv = StratifiedKFold(n_splits= 5 ,random_state=0,shuffle=True)\n",
    "clf = GridSearchCV(pipeline, parameters,n_jobs=-1, verbose=1, cv = 5)\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.637 (std: 0.008)\n",
      "Parameters: {'clf__C': 10, 'clf__gamma': 1}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.637 (std: 0.008)\n",
      "Parameters: {'clf__C': 100, 'clf__gamma': 1}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.628 (std: 0.012)\n",
      "Parameters: {'clf__C': 1, 'clf__gamma': 1}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.622 (std: 0.009)\n",
      "Parameters: {'clf__C': 10, 'clf__gamma': 0.1}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.620 (std: 0.008)\n",
      "Parameters: {'clf__C': 100, 'clf__gamma': 0.1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "            \n",
    "report(clf.cv_results_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.025, 0.1, 1, 10, 100)}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed: 12.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 799.559s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vect', HashingVectorizer(\n",
    "                        lowercase = True,\n",
    "                        stop_words= 'english',\n",
    "                        token_pattern= r'(?u)\\b[a-zA-Z_][a-zA-Z_]+\\b',\n",
    "                        n_features = 10000\n",
    "                        )),\n",
    "                \n",
    "                ('clf', SVC( kernel = 'linear' )),\n",
    "                ])\n",
    "parameters = { \n",
    "                'clf__C': (0.025, 0.1, 1, 10, 100),    \n",
    "                }    \n",
    "#SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "#  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#  tol=0.001, verbose=False)\n",
    "\n",
    "cv = StratifiedKFold(n_splits= 5 ,random_state=0,shuffle=True)\n",
    "clf = GridSearchCV(pipeline, parameters,n_jobs=-1, verbose=1, cv = 5)\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.619 (std: 0.006)\n",
      "Parameters: {'clf__C': 1}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.581 (std: 0.014)\n",
      "Parameters: {'clf__C': 10}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.570 (std: 0.015)\n",
      "Parameters: {'clf__C': 100}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.467 (std: 0.004)\n",
      "Parameters: {'clf__C': 0.1}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.120 (std: 0.014)\n",
      "Parameters: {'clf__C': 0.025}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "            \n",
    "report(clf.cv_results_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM accuracy:   0.591\n",
      "RBF SVM accuracy:   0.066\n"
     ]
    }
   ],
   "source": [
    "names = [ \"Linear SVM\", \"RBF SVM\"]\n",
    "\n",
    "vectorizer = HashingVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform( X_train )\n",
    "Xt= vectorizer.transform( X_test )\n",
    "clf = [\n",
    "    SVC(kernel=\"linear\", C=1),\n",
    "    SVC(gamma=10, C=1),\n",
    "    ]\n",
    "for name, clf in zip(names,clf):\n",
    "    clf.fit(X, y_train)\n",
    "    pred = clf.predict(Xt)\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"%s accuracy:   %0.3f\" % (name,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
