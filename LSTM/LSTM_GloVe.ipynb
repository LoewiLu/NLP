{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "9.89s get data package ：）\n",
      "Data prepared ：）\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "from feature_extraction import __get\n",
    "import numpy as np\n",
    "\n",
    "path = r'/Users/loewi/Documents/Pre_Learn/classification/20news-bydate/'\n",
    "os.chdir(path)\n",
    "#print(os.getcwd())\n",
    "\n",
    "print('Preparing data...')\n",
    "\n",
    "t0 = time() \n",
    "\n",
    "newsgroups_train = __get('20news-bydate-train')\n",
    "newsgroups_test = __get('20news-bydate-test')\n",
    "\n",
    "duration = time() - t0\n",
    "print('%0.2fs get data package ：）'%duration)\n",
    "\n",
    "data_train,data_test = newsgroups_train['data'], newsgroups_test['data'] #list of strings\n",
    "label_train, label_test = newsgroups_train['docs'], newsgroups_test['docs'] #array\n",
    "print('Data prepared ：）')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors...\n",
      "400000 word vectors prepared ：）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors...')\n",
    "\n",
    "words_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding='utf-8')\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    vector = np.asarray(word_vector[1:], dtype='float32')\n",
    "    words_index[word] = vector\n",
    "f.close()\n",
    " \n",
    "print('%s word vectors prepared ：）'%len(words_index)) #400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Found 90215 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#features\n",
    "\n",
    "print('Tokenizing...')\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "#keras.preprocessing.text.Tokenizer(num_words=None, #None或整数(最常见的)\n",
    "#                                   filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n',\n",
    "#                                   lower=True,\n",
    "#                                   split=\" \",\n",
    "#                                   char_level=False #char_level: 如果为 True, 每个字符将被视为一个标记\n",
    "#                                   )\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "sequences = tokenizer.texts_to_sequences(data_train)#返回值：2Dlist，每个list对应于一段输入文本\n",
    "tokenizer.fit_on_texts(data_test)\n",
    "sequences_test = tokenizer.texts_to_sequences(data_test)\n",
    "\n",
    "word_index = tokenizer.word_index #dict{key= word, value = 排名或者索引(从1开始)}\n",
    "print('Found %s unique tokens.'%len(word_index))\n",
    "\n",
    "#word_counts:字典，将单词（字符串）映射为它们在训练期间出现的次数。仅在调用fit_on_texts之后设置。\n",
    "#word_docs: 字典，将单词（字符串）映射为它们在训练期间所出现的文档或文本的数量。仅在调用fit_on_texts之后设置。\n",
    "#word_index: 字典，将单词（字符串）映射为它们的排名或者索引。仅在调用fit_on_texts之后设置。\n",
    "#document_count: 整数。分词器被训练的文档（文本或者序列）数量。仅在调用fit_on_texts或fit_on_sequences之后设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training data (11314, 1000)\n",
      "shape of training labels (11314, 20)\n",
      "shape of testing data (7532, 1000)\n",
      "shape of testing labels (7532, 20)\n"
     ]
    }
   ],
   "source": [
    "#准备训练用数据\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "#keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32'\n",
    "#                                           padding='pre', truncating='pre', value=0.)\n",
    "#sequences：浮点数或整数构成的两层嵌套列表 \n",
    "#maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0 \n",
    "#padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "#truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "#value：浮点数，此值将在填充时代替默认的填充值0\n",
    "\n",
    "X_train = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)#长度不足1000的用0填充(前端填充)\n",
    "X_test = pad_sequences(sequences_test, maxlen = MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "#to_categorical(y, num_classes=None) \n",
    "#y: 类别向量，num_classes:总共类别数\n",
    "\n",
    "y_train = to_categorical(label_train) #扩列，总类别20列\n",
    "y_test = to_categorical(label_test)\n",
    "\n",
    "print('shape of training data',X_train.shape)\n",
    "print('shape of training labels',y_train.shape)\n",
    "print('shape of testing data',X_test.shape)\n",
    "print('shape of testing labels',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 2262 validation samples \n"
     ]
    }
   ],
   "source": [
    "# split the training data for fun or simply use the paremeter in fit(validation_split=0.2)\n",
    "index = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(index)\n",
    "X_train = X_train[index]\n",
    "y_train = y_train[index]\n",
    "num_validation_samples = int(0.2*X_train.shape[0])\n",
    "print('split %d validation samples '%num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of real training data (9052, 1000)\n",
      "shape of real training labels (9052, 20)\n",
      "shape of validatation training data (2262, 1000)\n",
      "shape of validatation training labels (2262, 20)\n"
     ]
    }
   ],
   "source": [
    "X_train_split = X_train[:-nb_validation_samples]\n",
    "y_train_split = y_train[:-nb_validation_samples]\n",
    "X_train_val = X_train[-nb_validation_samples:]\n",
    "y_train_val = y_train[-nb_validation_samples:]\n",
    "\n",
    "print('shape of real training data',X_train_split.shape)\n",
    "print('shape of real training labels',y_train_split.shape)\n",
    "print('shape of validatation training data',X_train_val.shape)\n",
    "print('shape of validatation training labels',y_train_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embedding matrix: (20001, 100)\n"
     ]
    }
   ],
   "source": [
    "#把有效出现次数在前面的通过GloVe生成的字典，以及本身所有的Token串进行比对，得到出现在训练集中每个词的词向量\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "num_words = min(MAX_NUM_WORDS,len(word_index))\n",
    "embedding_matrix = np.zeros((num_words +1,EMBEDDING_DIM))\n",
    "for word,i in word_index.items():\n",
    "    if i>MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = words_index.get(word) #array\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector        \n",
    "print('shape of embedding matrix:',embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model completed ：）\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         2000100   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                40        \n",
      "=================================================================\n",
      "Total params: 2,080,641\n",
      "Trainable params: 2,000,241\n",
      "Non-trainable params: 80,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model LSTM\n",
    "\n",
    "#keras.layers.embeddings.Embedding(\n",
    "#                                input_dim, output_dim, \n",
    "#                                embeddings_initializer='uniform', embeddings_regularizer=None, \n",
    "#                                activity_regularizer=None, embeddings_constraint=None, \n",
    "#                                mask_zero=False, input_length=None\n",
    "#                                )\n",
    "#Embedding层只能作为模型的第一层\n",
    "embedding_layer = Embedding(num_words + 1, #input_dim：大或等于0的整数，字典长度，即输入数据最大下标+1\n",
    "                            EMBEDDING_DIM,#output_dim：大于0的整数，代表全连接嵌入的维度\n",
    "                            weights=[embedding_matrix], #(20001, 100)\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, \n",
    "#当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。\n",
    "                            )\n",
    "print('Building model...')\n",
    "\n",
    "model = Sequential() #序贯模型是多个网络层的线性堆叠，也就是“一条路走到黑”\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))  #100维\n",
    "model.add(Dense(1))#dense层，大于0的整数，代表该层的输出维度\n",
    "model.add(Activation('sigmoid')) #激活层是对一个层的输出施加激活函数\n",
    "model.add(Dense(len(newsgroups_train['classes']), activation='softmax'))#Softmax将连续数值转化成相对概率\n",
    "model.layers[1].trainable=False\n",
    "\n",
    "print('Model completed ：）')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编译\n",
    "\n",
    "#compile(self, optimizer, loss, metrics=None, \n",
    "#        loss_weights=None, sample_weight_mode=None, \n",
    "#        weighted_metrics=None, target_tensors=None)\n",
    "\n",
    "model.compile(\n",
    "            optimizer='adam',#优化器\n",
    "            loss='binary_crossentropy',#损失函数\n",
    "            metrics=['accuracy'],#指标列表\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 9052 samples, validate on 2262 samples\n",
      "Epoch 1/5\n",
      "9052/9052 [==============================] - 287s 32ms/step - loss: 0.1985 - acc: 0.9500 - val_loss: 0.1985 - val_acc: 0.9500\n",
      "Epoch 2/5\n",
      "9052/9052 [==============================] - 284s 31ms/step - loss: 0.1982 - acc: 0.9500 - val_loss: 0.1983 - val_acc: 0.9500\n",
      "Epoch 3/5\n",
      "9052/9052 [==============================] - 296s 33ms/step - loss: 0.1980 - acc: 0.9500 - val_loss: 0.1981 - val_acc: 0.9500\n",
      "Epoch 4/5\n",
      "9052/9052 [==============================] - 298s 33ms/step - loss: 0.1976 - acc: 0.9500 - val_loss: 0.1976 - val_acc: 0.9500\n",
      "Epoch 5/5\n",
      "9052/9052 [==============================] - 275s 30ms/step - loss: 0.1968 - acc: 0.9500 - val_loss: 0.1969 - val_acc: 0.9500\n",
      "7532/7532 [==============================] - 39s 5ms/step\n",
      "Loss: 0.19801047293195304\n",
      "Accuracy: 0.949999988079071\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "#fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, \n",
    "#        callbacks=None, validation_split=0.0, validation_data=None, \n",
    "#        shuffle=True, class_weight=None, sample_weight=None, \n",
    "#        initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "#batch_size:整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步\n",
    "#epochs：整数，训练终止时的epoch值，训练将在达到该epoch值时停止，当没有设置\n",
    "#validation_data：形式为（X，y）或（X，y，sample_weights）的tuple，是指定的验证集。此参数将覆盖validation_spilt\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(X_train_split, y_train_split, batch_size=batch_size, epochs=5, validation_data=(X_train_val,y_train_val))\n",
    "\n",
    "#evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "#x：输入数据，与fit一样，是numpy array或numpy array的list\n",
    "#y：标签，numpy array\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Loss:',loss) \n",
    "print('Accuracy:',acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('my_model1.h5')\n",
    "#model = load_model('my_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotted!\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png')\n",
    "print('plotted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
