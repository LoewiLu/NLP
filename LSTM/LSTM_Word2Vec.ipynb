{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Nov  4 15:44:42 2018\n",
    "Word2vec embeddings: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "@author: loewi\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "from feature_extraction import __get\n",
    "import numpy as np\n",
    "\n",
    "path = r'/Users/loewi/Documents/Pre_Learn/classification/20news-bydate/'\n",
    "os.chdir(path)\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "9.54s get data package ：）\n",
      "Data prepared ：）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Preparing data...')\n",
    "\n",
    "t0 = time() \n",
    "\n",
    "newsgroups_train = __get('20news-bydate-train')\n",
    "newsgroups_test = __get('20news-bydate-test')\n",
    "\n",
    "duration = time() - t0\n",
    "print('%0.2fs get data package ：）'%duration)\n",
    "\n",
    "data_train,data_test = newsgroups_train['data'], newsgroups_test['data'] #list of strings\n",
    "label_train, label_test = newsgroups_train['docs'], newsgroups_test['docs'] #array\n",
    "print('Data prepared ：）')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning...\n",
      "11314 sentences \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "print('Cleaning...')\n",
    "\n",
    "def token_pattern(string):\n",
    "    token_pattern = re.compile(r'(?u)\\b[a-zA-Z_][a-zA-Z_]+\\b')\n",
    "    return token_pattern.findall(string)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def clean(string):        \n",
    "    result = [ _ for _ in token_pattern(string) if not _ in stop]\n",
    "    return result\n",
    "\n",
    "sentences = []\n",
    "for data in data_train:    \n",
    "    sentences.append(clean(data))\n",
    "    \n",
    "print('%d sentences '%len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embedding matrix: (20001, 100)\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, \n",
    "#            window=5, min_count=5, max_vocab_size=None, sample=0.001, \n",
    "#            seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, \n",
    "#            ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, i\n",
    "#            ter=5, null_word=0, trim_rule=None, sorted_vocab=1, \n",
    "#            batch_words=10000, compute_loss=False, callbacks=(), \n",
    "#            max_final_vocab=None)\n",
    "model = Word2Vec(sentences,min_count=1)\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100    \n",
    "num_words = min(MAX_NUM_WORDS,len(model.wv.vocab))\n",
    "\n",
    "embedding_matrix = np.zeros((num_words +1,EMBEDDING_DIM))\n",
    "for i in range(num_words):\n",
    "    if i>MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = model.wv[model.wv.index2word[i]] \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('shape of embedding matrix:',embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Found 90215 unique tokens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Tokenizing...')\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "#keras.preprocessing.text.Tokenizer(num_words=None, #None或整数(最常见的)\n",
    "#                                   filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n',\n",
    "#                                   lower=True,\n",
    "#                                   split=\" \",\n",
    "#                                   char_level=False #char_level: 如果为 True, 每个字符将被视为一个标记\n",
    "#                                   )\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "sequences = tokenizer.texts_to_sequences(data_train)#返回值：2Dlist，每个list对应于一段输入文本\n",
    "tokenizer.fit_on_texts(data_test)\n",
    "sequences_test = tokenizer.texts_to_sequences(data_test)\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index #dict{key= word, value = 排名或者索引(从1开始)}\n",
    "print('Found %s unique tokens.'%len(word_index))\n",
    "#word_counts:字典，将单词（字符串）映射为它们在训练期间出现的次数。仅在调用fit_on_texts之后设置。\n",
    "#word_docs: 字典，将单词（字符串）映射为它们在训练期间所出现的文档或文本的数量。仅在调用fit_on_texts之后设置。\n",
    "#word_index: 字典，将单词（字符串）映射为它们的排名或者索引。仅在调用fit_on_texts之后设置。\n",
    "#document_count: 整数。分词器被训练的文档（文本或者序列）数量。仅在调用fit_on_texts或fit_on_sequences之后设置。\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training data (11314, 1000)\n",
      "shape of training labels (11314, 20)\n",
      "shape of testing data (7532, 1000)\n",
      "shape of testing labels (7532, 20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "#keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32'\n",
    "#                                           padding='pre', truncating='pre', value=0.)\n",
    "#sequences：浮点数或整数构成的两层嵌套列表 \n",
    "#maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0 \n",
    "#padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "#truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "#value：浮点数，此值将在填充时代替默认的填充值0\n",
    "X_train = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)#长度不足1000的用0填充(前端填充)\n",
    "X_test = pad_sequences(sequences_test, maxlen = MAX_SEQUENCE_LENGTH) \n",
    "#to_categorical(y, num_classes=None) \n",
    "#y: 类别向量，num_classes:总共类别数\n",
    "y_train = to_categorical(label_train) #扩列，总类别20列\n",
    "y_test = to_categorical(label_test)\n",
    "\n",
    "print('shape of training data',X_train.shape)\n",
    "print('shape of training labels',y_train.shape)\n",
    "print('shape of testing data',X_test.shape)\n",
    "print('shape of testing labels',y_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model completed ：）\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1000, 100)         2000100   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                40        \n",
      "=================================================================\n",
      "Total params: 2,080,641\n",
      "Trainable params: 2,000,241\n",
      "Non-trainable params: 80,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "#keras.layers.embeddings.Embedding(\n",
    "#                                input_dim, output_dim, \n",
    "#                                embeddings_initializer='uniform', embeddings_regularizer=None, \n",
    "#                                activity_regularizer=None, embeddings_constraint=None, \n",
    "#                                mask_zero=False, input_length=None\n",
    "#                                )\n",
    "#Embedding层只能作为模型的第一层\n",
    "embedding_layer = Embedding(num_words + 1, #input_dim：大或等于0的整数，字典长度，即输入数据最大下标+1\n",
    "                            EMBEDDING_DIM,#output_dim：大于0的整数，代表全连接嵌入的维度\n",
    "                            weights=[embedding_matrix], #(20001, 100)\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, \n",
    "#当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。\n",
    "                            )\n",
    "print('Building model...')\n",
    "\n",
    "model = Sequential() #序贯模型是多个网络层的线性堆叠，也就是“一条路走到黑”\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))  #100维\n",
    "model.add(Dense(1))#dense层，大于0的整数，代表该层的输出维度\n",
    "model.add(Activation('sigmoid')) #激活层是对一个层的输出施加激活函数\n",
    "model.add(Dense(len(newsgroups_train['classes']), activation='softmax'))#Softmax将连续数值转化成相对概率\n",
    "model.layers[1].trainable=False\n",
    "\n",
    "print('Model completed ：）')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/5\n",
      "9051/9051 [==============================] - 289s 32ms/step - loss: 0.1970 - acc: 0.9500 - val_loss: 0.2138 - val_acc: 0.9500\n",
      "Epoch 2/5\n",
      "9051/9051 [==============================] - 294s 33ms/step - loss: 0.1937 - acc: 0.9500 - val_loss: 0.2322 - val_acc: 0.9500\n",
      "Epoch 3/5\n",
      "9051/9051 [==============================] - 290s 32ms/step - loss: 0.1913 - acc: 0.9500 - val_loss: 0.2551 - val_acc: 0.9500\n",
      "Epoch 4/5\n",
      "9051/9051 [==============================] - 290s 32ms/step - loss: 0.1895 - acc: 0.9500 - val_loss: 0.2756 - val_acc: 0.9500\n",
      "Epoch 5/5\n",
      "9051/9051 [==============================] - 286s 32ms/step - loss: 0.1885 - acc: 0.9500 - val_loss: 0.2918 - val_acc: 0.9500\n",
      "7532/7532 [==============================] - 37s 5ms/step\n",
      "Loss: 0.2090653621700933\n",
      "Accuracy: 0.949999988079071\n"
     ]
    }
   ],
   "source": [
    "#编译\n",
    "#compile(self, optimizer, loss, metrics=None, \n",
    "#        loss_weights=None, sample_weight_mode=None, \n",
    "#        weighted_metrics=None, target_tensors=None)\n",
    "model.compile(\n",
    "            optimizer='adam',#优化器\n",
    "            loss='binary_crossentropy',#损失函数\n",
    "            metrics=['accuracy'],#指标列表\n",
    "            )\n",
    "\n",
    "print('Training...')\n",
    "#fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, \n",
    "#        callbacks=None, validation_split=0.0, validation_data=None, \n",
    "#        shuffle=True, class_weight=None, sample_weight=None, \n",
    "#        initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "#batch_size:整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步\n",
    "#epochs：整数，训练终止时的epoch值，训练将在达到该epoch值时停止，当没有设置\n",
    "#validation_data：形式为（X，y）或（X，y，sample_weights）的tuple，是指定的验证集。此参数将覆盖validation_spilt\n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=5, validation_split = 0.2)\n",
    "\n",
    "#evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "#x：输入数据，与fit一样，是numpy array或numpy array的list\n",
    "#y：标签，numpy array\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Loss:',loss)\n",
    "print('Accuracy:',acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model_Word2Vec_LSTM.h5')\n",
    "#model = load_model('my_model_Word2Vec_LSTM.h5') \n",
    "print('Model saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
